# PredicciÃ³n de Precios de Diamantes ðŸ’Ž

Este repositorio contiene la implementaciÃ³n y anÃ¡lisis de distintos modelos de **Machine Learning** para predecir el **precio de diamantes** a partir de sus caracterÃ­sticas fÃ­sicas (`carat`, `depth`, `x`, `y`, `z`).  
Se trabaja con el dataset [Diamonds de Kaggle](https://www.kaggle.com/datasets/shivam2503/diamonds), que incluye mÃ¡s de 50,000 registros.

---

## Archivos del repositorio

### Implementaciones
- **`Gradient_Descent_Diamantes.md`** â†’ ExplicaciÃ³n del algoritmo de **RegresiÃ³n Lineal con Descenso de Gradiente**, implementado desde cero en Python, con pasos de preprocesamiento, estandarizaciÃ³n y entrenamiento.  
- **`RandomForest_Diamantes_Diamantes.md`** â†’ ImplementaciÃ³n del modelo **Random Forest Regressor**, incluyendo justificaciÃ³n teÃ³rica, preprocesamiento, entrenamiento, mÃ©tricas y generaciÃ³n de grÃ¡ficas de evaluaciÃ³n.  

### AnÃ¡lisis de resultados
- **`AnÃ¡lisis_Resultados_Gradient_Descent.md`** â†’ Resultados de la regresiÃ³n lineal con descenso de gradiente (RÂ², MSE, interpretaciÃ³n de pesos y sesgo).  
- **`AnÃ¡lisis_Resultados_Random_Forest.md`** â†’ AnÃ¡lisis detallado de varias iteraciones del Random Forest, con ajuste de hiperparÃ¡metros, diagnÃ³stico de bias/varianza, grÃ¡ficas de residuales, calibraciÃ³n y comparaciÃ³n entre train/validation/test.  

---

## Modelos implementados

### RegresiÃ³n Lineal con Descenso de Gradiente
- Implementado sin librerÃ­as de ML, solo con **Python y NumPy**.  
- RÂ² â‰ˆ **0.82** en train y test.  
- `carat` y dimensiones fÃ­sicas (`x`, `y`, `z`) son las variables mÃ¡s influyentes.  
- Buen balance entre entrenamiento y prueba â†’ sin overfitting ni underfitting.

### Random Forest Regressor
- Modelo de ensamble de Ã¡rboles, robusto a no linealidades y outliers.  
- RÂ² en validaciÃ³n y prueba â‰ˆ **0.88â€“0.89** tras ajuste de hiperparÃ¡metros.  
- Ligero overfitting en la primera configuraciÃ³n, corregido en iteraciones posteriores.  
- Mejor desempeÃ±o en diamantes pequeÃ±os y medianos; mayor error en diamantes grandes.  

---

## Objetivo
Comparar enfoques de **modelos lineales** (Gradient Descent) vs. **modelos de ensamble** (Random Forest), evaluando:
- PrecisiÃ³n predictiva (RÂ², MSE, MAE).
- Sesgo (bias).
- Capacidad de generalizaciÃ³n.  
- Comportamiento del error en funciÃ³n del tamaÃ±o del diamante.

---

## Resultados clave
- **Gradient Descent:** simple, interpretable, y buen ajuste con RÂ²â‰ˆ0.82.  
- **Random Forest:** mÃ¡s complejo, mejor rendimiento con RÂ²â‰ˆ0.89 y menor sesgo.  
- Ambos modelos muestran que los **diamantes grandes** son los mÃ¡s difÃ­ciles de predecir con exactitud.  

---

## TecnologÃ­as usadas
- Python (NumPy, Pandas, Matplotlib, Scikit-learn).  
- Dataset: Diamonds (Kaggle).  
- Visualizaciones: Parity plots, histogramas de residuales, curvas de error.  

---

## ConclusiÃ³n
Este proyecto demuestra cÃ³mo distintos enfoques de Machine Learning capturan la relaciÃ³n entre las caracterÃ­sticas fÃ­sicas de un diamante y su precio.  
- El **Gradient Descent** ofrece interpretabilidad y simplicidad.  
- El **Random Forest** logra mayor precisiÃ³n y estabilidad, a costa de menor interpretabilidad.  

---

## EvaluaciÃ³n del desempeÃ±o

- **Conjuntos usados:**  
Ambos modelos fueron evaluados con un **conjunto de validaciÃ³n** y un **conjunto de prueba externo**, confirmando su capacidad de generalizaciÃ³n.

- **Train (`train_diamonds.csv`)**  
  Dataset principal de entrenamiento.  
  - Limpieza: `x, y, z > 0` y `price < 20000` para eliminar valores invÃ¡lidos y outliers.  
  - DivisiÃ³n: **80% Train** y **20% Validation** mediante `train_test_split`.  

- **Validation (20% del train)**  
  Subconjunto interno usado para ajustar hiperparÃ¡metros y controlar el sobreajuste.  

- **Test (`test_diamonds.csv`)**  
  Conjunto externo e independiente para la **evaluaciÃ³n final**.  
  - Mismos filtros que en train.  
  - Se genera `rf_predictions_test.csv` con precios reales y predichos.  
  

- **Bias (sesgo):**  
  - *Gradient Descent:* Bajo â†’ errores centrados en 0, sin sobre/subestimaciÃ³n clara.  
  - *Random Forest:* Bajo â†’ sesgo prÃ¡cticamente nulo en validaciÃ³n y prueba.  

- **Varianza:**  
  - *Gradient Descent:* Media â†’ estable entre train y test, sin grandes caÃ­das de rendimiento.  
  - *Random Forest:* Media-Alta en la primera iteraciÃ³n (ligero overfit), reducida a **media** tras ajustes de hiperparÃ¡metros.  

- **Nivel de ajuste:**  
  - *Gradient Descent:* **Fit balanceado**, sin underfitting ni overfitting.  
  - *Random Forest:* Inicialmente **ligero overfitting**, corregido en iteraciones posteriores â†’ modelo mÃ¡s **equilibrado**.  

- **RegularizaciÃ³n aplicada:**  
  - *Gradient Descent:* La estandarizaciÃ³n de variables ayuda a estabilizar el entrenamiento.  
  - *Random Forest:* Se aplicaron tÃ©cnicas de regularizaciÃ³n ajustando hiperparÃ¡metros (`max_depth`, `min_samples_leaf`, `min_samples_split`, `ccp_alpha`), lo que redujo el sobreajuste y mejorÃ³ la estabilidad.  
